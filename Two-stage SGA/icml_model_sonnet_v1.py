'''
H^T * epsi - ( H^T - diag(H^T))
'''
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import math
import os
import numpy as np
import sonnet as snt
import tensorflow as tf
import kfac

import matplotlib.pyplot as plt
import scipy as sp
#@title Defining the SGA Optimiser

def list_divide_scalar(xs, y):
  return [x / y for x in xs]


def list_subtract(xs, ys):
  return [x - y for (x, y) in zip(xs, ys)]

def list_add(xs, ys):
  return [x + y for (x, y) in zip(xs, ys)]

def jacobian_vec(ys, xs, vs):
  return kfac.utils.fwd_gradients(
  #return fwd_gradients(
      ys, xs, grad_xs=vs, stop_gradients=xs)


def jacobian_transpose_vec(ys, xs, vs):
  dydxs = tf.gradients(ys, xs, grad_ys=vs, stop_gradients=xs)
  dydxs = [
      tf.zeros_like(x) if dydx is None else dydx for x, dydx in zip(xs, dydxs)
  ]

  hessian_diagonal = []
  for x, y, v in zip(xs, ys, vs):
    hessian_diagonal.append(tf.gradients(y, x, stop_gradients=x, grad_ys = v )[0])
  '''
  for x, y in zip(xs, ys):
    hessian_diagonal.append(tf.gradients(y, x, stop_gradients=x)[0])
    #print(x)
  hessian = tf.gradients(ys, xs, stop_gradients=xs)# grad_xs = epsilon
  print("hessian-----------------")
  print(hessian)
  print("hessian_diagonal-----------------")
  print(hessian_diagonal)
  hessianExcluseDiag = list_subtract(hessian, hessian_diagonal)
  dysdx_diagonal=list_multiply(hessianExcluseDiag, vs)
  dysdx_diagonal = [
      tf.zeros_like(x) if dydx is None else dydx for x, dydx in zip(xs, dysdx_diagonal)
  ]

  #hessian_tf = tf.hessians(ys, xs)
  #print("hessian_tf------------------")
  #print(hessian_tf.shape)
  '''
  #return dydxs, dysdx_diagonal
  return dydxs, hessian_diagonal


def _dot(x, y):
  dot_list = []
  for xx, yy in zip(x, y):
    dot_list.append(tf.reduce_sum(xx * yy))
  return tf.add_n(dot_list)

def list_multiply(xs, ys):
  dot_list = []
  for xx, yy in zip(xs,ys):
    dot_list.append(tf.multiply(xx,yy))
  return dot_list
def scalar_list_multiply(x, ys):
  dot_list = []
  for yy in ys:
    dot_list.append(tf.multiply(x,yy))
  return dot_list

def fwd_gradients(ys, xs, grad_xs=None, stop_gradients=None):
  """Compute forward-mode gradients."""
  # See b/37888268.

  # This version of forward-mode autodiff is based on code by Tim Cooijmans
  # and handles list arguments and certain special cases such as when the
  # ys doesn't depend on one or more of the xs, and when tf.IndexedSlices are
  # generated by the first tf.gradients call.

  us = [tf.zeros_like(y) + float("nan") for y in ys]
  #print("xs----------------")
  #print(xs)
  #print("ys----------------")
  #print(ys)
  #print("us----------------")
  #print(us)
  dydxs = tf.gradients(ys, xs, grad_ys=us, stop_gradients=stop_gradients) #stop_gradients == xs
  #print("dydxs")
  # Deal with strange types that tf.gradients returns but can't
  # deal with.
  dydxs = [
      tf.convert_to_tensor(dydx) if isinstance(dydx, tf.IndexedSlices) else dydx
      for dydx in dydxs
  ]
  dydxs = [
      tf.zeros_like(x) if dydx is None else dydx for x, dydx in zip(xs, dydxs)
  ]

  dysdx = tf.gradients(dydxs, us, grad_ys=grad_xs)# grad_xs = epsilon
  hessian_diagonal = []
  for u, dydx in zip(us, dydxs):
    hessian_diagonal.append(tf.gradients(dydx, u)[0])
    #print(x)
  hessian = tf.gradients(dydxs, us)# grad_xs = epsilon
  print("hessian-----------------")
  print(hessian)
  print("hessian_diagonal-----------------")
  print(hessian_diagonal)
  hessianExcluseDiag = list_subtract(hessian, hessian_diagonal)
  dysdx_diagonal=list_multiply(hessianExcluseDiag, grad_xs)
  #dysdx_diagonal_1=scalar_list_multiply(0.2, dysdx_diagonal)
  #dysdx_diagonal_1=scalar_list_multiply(0.2, dysdx_diagonal)
  #print("dydxs---------------")
  #print(dydxs)
  #print("dysdx---------------")
  #print(dysdx)
  #print("dysdx_diagonal-----------------")
  #print(dysdx_diagonal)

  return dysdx,dysdx_diagonal
  #return dysdx,dysdx


class SymplecticOptimizer_ICML(tf.train.Optimizer):
  """Optimizer that corrects for rotational components in gradients."""

  def __init__(self,
               learning_rate,
               reg_params=1.,
               use_signs=True,
               use_locking=False,
               name='symplectic_optimizer'):
    super(SymplecticOptimizer_ICML, self).__init__(
        use_locking=use_locking, name=name)
    self._gd = tf.train.RMSPropOptimizer(learning_rate)
    self._reg_params = reg_params
    self._use_signs = use_signs

  def compute_gradients(self,
                        loss,
                        var_list=None,
                        gate_gradients=tf.train.Optimizer.GATE_OP,
                        aggregation_method=None,
                        colocate_gradients_with_ops=False,
                        grad_loss=None):
    return self._gd.compute_gradients(loss, var_list, gate_gradients,
                                      aggregation_method,
                                      colocate_gradients_with_ops, grad_loss)
  def apply_gradients(self, grads_and_vars, global_step=None, name=None):
    grads, vars_ = zip(*grads_and_vars)
    n = len(vars_)
    print("variables number: %s" %n)
    #print(vars_)
    #for var in vars_:
    #    print("%s:%s" %(var.name, var.shape))
    #for grad in grads:
    #    print("%s:%s" %(grad.name, grad.shape))
    #print(grads[1].shape)
    h_v = jacobian_vec(grads, vars_, grads)
    #h_v_double_diagonal = list_add(h_v, h_v_diagonal) #A * epislon
    ht_v, ht_v_diagonal  = jacobian_transpose_vec(grads, vars_, grads)
    at_v = list_divide_scalar(list_subtract(ht_v, h_v), 2.) #A * epislon
    #at_v = list_subtract(at_v, ht_v_diagonal)
    at_v = list_add(at_v, ht_v_diagonal) #v2
    
#aht_v = list_multiply(at_v, ht_v)
    #print("hessian %s" %hessian[1].shape)
    #print(type(h_v))
    #print(h_v[0].shape)
    #print(type(ht_v))
    #print(type(at_v))
    #print(len(ht_v))
    #print(type(ht_v[0]))
    #print((ht_v[0].shape))
    #print((at_v[0].shape))
    if self._use_signs:
      # grad_dot_h = _dot(grads, ht_v)
      # at_v_dot_h = _dot(at_v, ht_v)
      # mult = grad_dot_h * at_v_dot_h
      # lambda_ = tf.sign(mult / n + 0.1) * self._reg_params
      print("Align!!!!!!!!!!!!!!!")
      lambda_ = self._reg_params
      #pass
    else:
      print("NonAlign!!!!!!!!!!!!!!!")
      lambda_ = self._reg_params
    apply_vec = [(g + lambda_ * ag, x)
                 for (g, ag, x) in zip(grads, at_v, vars_)
                 if at_v is not None]
    return self._gd.apply_gradients(apply_vec, global_step, name)
#@title An MLP Sonnet module

class MLP(snt.AbstractModule):
  """An MLP with hidden layers of the same width as the input."""

  def __init__(self, depth, hidden_size, out_dim, name='SimpleNet'):
    super(MLP, self).__init__(name=name)
    self._depth = depth
    self._hidden_size = hidden_size
    self._out_dim = out_dim

  def _build(self, input):
    h = input
    for i in range(self._depth):
      h = tf.nn.relu(snt.Linear(self._hidden_size)(h))
    return snt.Linear(self._out_dim)(h)

def reset_and_build_graph(
    depth, width, x_real_builder, z_dim, batch_size, learning_rate, mode, align=True):
  tf.reset_default_graph()

  x_real = x_real_builder(batch_size)
  print("x_real: %s" %x_real.get_shape()) #256 * 2
  x_dim = x_real.get_shape().as_list()[1]
  generator = MLP(depth, width, x_dim, 'generator')
  discriminator = MLP(depth, width, 1, 'discriminator')
  z = tf.random_normal([batch_size, z_dim])
  x_fake = generator(z)
  print("x_fake: %s" %x_fake.shape) #256 * 2
  disc_out_real = discriminator(x_real)
  disc_out_fake = discriminator(x_fake)
  print("disc_out_real: %s" %disc_out_real) #256 * 1
  # Loss
  disc_loss_real = tf.reduce_mean(
      tf.nn.sigmoid_cross_entropy_with_logits(
          logits=disc_out_real, labels=tf.ones_like(disc_out_real)))
  disc_loss_fake = tf.reduce_mean(
      tf.nn.sigmoid_cross_entropy_with_logits(
          logits=disc_out_fake, labels=tf.zeros_like(disc_out_fake)))
  disc_loss = disc_loss_real + disc_loss_fake

  gen_loss = tf.reduce_mean(
      tf.nn.sigmoid_cross_entropy_with_logits(
          logits=disc_out_fake, labels=tf.ones_like(disc_out_fake)))
  gen_vars = generator.variable_scope.trainable_variables()
  disc_vars = discriminator.variable_scope.trainable_variables()
  # Compute gradients
  xs = disc_vars + gen_vars
  disc_grads = tf.gradients(disc_loss, disc_vars)
  gen_grads = tf.gradients(gen_loss, gen_vars)

  Xi = disc_grads + gen_grads
  apply_vec = list(zip(Xi, xs))

  if mode == 'RMS':
    optimizer = tf.train.RMSPropOptimizer(learning_rate)
  elif mode == 'ICML':
    optimizer = SymplecticOptimizer_ICML(learning_rate,use_signs=align)
  else:
    raise ValueError('Mode %s not recognised' % mode)

  with tf.control_dependencies([g for (g, v) in apply_vec]):
    train_op = optimizer.apply_gradients(apply_vec)

  init = tf.global_variables_initializer()
  
  return train_op, x_fake, z, init, disc_loss, gen_loss
